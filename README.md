# ğŸ§  Neuropathology Detection System

A comprehensive deep learning system for detecting neuropathological conditions from brain MRI scans. This project combines state-of-the-art computer vision techniques with an intuitive web interface, designed as an educational platform for learning deep learning from scratch.

[![TensorFlow](https://img.shields.io/badge/TensorFlow-2.15-orange.svg)](https://tensorflow.org)
[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://python.org)
[![Streamlit](https://img.shields.io/badge/Streamlit-1.29-red.svg)](https://streamlit.io)

## ğŸ“‹ Table of Contents

- [Overview](#overview)
- [Features](#features)
- [System Architecture](#system-architecture)
- [Installation](#installation)
- [Usage](#usage)
- [Dataset](#dataset)
- [Model Architecture](#model-architecture)
- [Training Pipeline](#training-pipeline)
- [Web Application](#web-application)
- [Deep Learning Concepts](#deep-learning-concepts)
- [Project Structure](#project-structure)
- [Results](#results)
- [Future Work](#future-work)
- [Contributing](#contributing)
- [License](#license)

## ğŸ¯ Overview

This project implements a complete neuropathology detection system powered by deep learning for medical image (MRI) analysis. It features:

- **Transfer Learning**: Leverages pre-trained models (EfficientNet, ResNet, VGG, MobileNet) trained on ImageNet
- **Fine-tuning**: Adapts general image features to medical imaging domain
- **Modern Web Interface**: Interactive Streamlit application for real-time predictions
- **Comprehensive Documentation**: Extensive theoretical explanations of deep learning concepts
- **Educational Focus**: Designed for learning deep learning from scratch with hands-on implementation

### Detected Conditions

The system can classify brain MRI scans into four categories:

1. **Glioma**: Brain tumors originating from glial cells
2. **Meningioma**: Tumors of the meninges (brain/spinal cord membranes)
3. **Pituitary Tumor**: Tumors of the pituitary gland
4. **Normal**: No pathological findings

## âœ¨ Features

### Technical Features

- **Multiple Model Architectures**: Support for EfficientNet, ResNet50, VGG16, MobileNetV2
- **Two-Stage Training**: Feature extraction followed by fine-tuning for optimal results
- **Data Augmentation**: Rotation, zoom, and flip transformations to improve generalization
- **Comprehensive Metrics**: Accuracy, precision, recall, F1-score, AUC-ROC
- **Model Callbacks**: Early stopping, learning rate scheduling, model checkpoints
- **Visualization Tools**: Training curves, confusion matrices, prediction confidence

### User Interface Features

- **Modern Web Design**: Clean, intuitive Streamlit interface
- **Real-time Predictions**: Upload MRI images and get instant diagnoses
- **Confidence Scores**: Detailed probability distributions for all classes
- **Interactive Visualizations**: Plotly charts for prediction analysis
- **Educational Content**: Built-in theory explanations and documentation
- **Responsive Layout**: Works on desktop and mobile devices

## ğŸ—ï¸ System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     User Interface Layer                      â”‚
â”‚                    (Streamlit Web App)                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Application Layer                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚ Image Upload â”‚  â”‚  Prediction  â”‚  â”‚Visualization â”‚      â”‚
â”‚  â”‚   Handler    â”‚  â”‚    Engine    â”‚  â”‚   Module     â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Deep Learning Layer                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚           Pre-trained Base Model                 â”‚        â”‚
â”‚  â”‚  (EfficientNet / ResNet / VGG / MobileNet)       â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚         Custom Classification Head               â”‚        â”‚
â”‚  â”‚  (Dense Layers + Dropout + Batch Norm)           â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Data Processing Layer                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚Image Loading â”‚  â”‚Preprocessing â”‚  â”‚ Augmentation â”‚      â”‚
â”‚  â”‚              â”‚  â”‚ & Resizing   â”‚  â”‚              â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Installation

### Prerequisites

- Python 3.8 or higher
- pip package manager
- (Optional) NVIDIA GPU with CUDA for faster training

### Step 1: Clone the Repository

```bash
git clone https://github.com/amanraj41/neuropathology-dl.git
cd neuropathology-dl
```

### Step 2: Create Virtual Environment (Recommended)

```bash
# Create virtual environment
python -m venv venv

# Activate virtual environment
# On Windows:
venv\Scripts\activate
# On Linux/Mac:
source venv/bin/activate
```

### Step 3: Install Dependencies

```bash
pip install -r requirements.txt
```

### Step 4: Verify Installation

```bash
python -c "import tensorflow as tf; print('TensorFlow version:', tf.__version__)"
python -c "import streamlit as st; print('Streamlit installed successfully')"
```

## ğŸ“– Usage

### Running the Web Application

Start the Streamlit web interface:

```bash
streamlit run app.py
```

The application will open in your default browser at `http://localhost:8501`.

### Training a Model

To train a model on your own dataset:

```bash
python train.py --data_dir /path/to/your/dataset --epochs_stage1 30 --epochs_stage2 20
```

#### Training Arguments

```bash
python train.py \
    --data_dir ./brain_mri_data \      # Path to dataset
    --base_model efficientnet \         # Model architecture
    --batch_size 32 \                   # Batch size
    --epochs_stage1 30 \                # Feature extraction epochs
    --epochs_stage2 20 \                # Fine-tuning epochs
    --learning_rate 0.001 \             # Initial learning rate
    --learning_rate_finetune 0.0001 \   # Fine-tuning learning rate
    --trainable_layers 20               # Layers to fine-tune
```

### Making Predictions

#### Using Python Script

```python
from src.models.neuropathology_model import NeuropathologyModel
from src.data.data_loader import MRIDataLoader
import numpy as np

# Load model
model = NeuropathologyModel()
model.load_model('models/final_model.h5')

# Load and preprocess image
loader = MRIDataLoader(img_size=(224, 224))
image = loader.load_and_preprocess_image('path/to/mri_scan.jpg')
image_batch = np.expand_dims(image, axis=0)

# Make prediction
predictions = model.predict(image_batch)
predicted_class, confidence = model.predict_class(image_batch)

print(f"Predicted: {predicted_class}, Confidence: {confidence}")
```

## ğŸ“Š Dataset

### Expected Dataset Structure

```
data/
â”œâ”€â”€ Glioma/
â”‚   â”œâ”€â”€ image1.jpg
â”‚   â”œâ”€â”€ image2.jpg
â”‚   â””â”€â”€ ...
â”œâ”€â”€ Meningioma/
â”‚   â”œâ”€â”€ image1.jpg
â”‚   â”œâ”€â”€ image2.jpg
â”‚   â””â”€â”€ ...
â”œâ”€â”€ Pituitary/
â”‚   â”œâ”€â”€ image1.jpg
â”‚   â”œâ”€â”€ image2.jpg
â”‚   â””â”€â”€ ...
â””â”€â”€ Normal/
    â”œâ”€â”€ image1.jpg
    â”œâ”€â”€ image2.jpg
    â””â”€â”€ ...
```

### Recommended Datasets

1. **Brain MRI Images for Brain Tumor Detection** (Kaggle)
   - ~3000 MRI images
   - 4 classes (Glioma, Meningioma, Pituitary, No Tumor)
   - Pre-processed and ready to use

2. **BraTS (Brain Tumor Segmentation)** Challenge Dataset
   - More advanced dataset with segmentation masks
   - Multiple imaging modalities

### Data Preprocessing

The system automatically handles:
- Resizing to 224Ã—224 pixels
- Normalization to [0, 1] range
- Data augmentation (training only):
  - Random rotation (Â±20Â°)
  - Random zoom (Â±10%)
  - Horizontal flip (50% probability)

## ğŸ§  Model Architecture

### Base Model Options

#### 1. EfficientNetB0 (Default - Recommended)

```
Parameters: ~5.3M
Advantages:
- Best accuracy/efficiency trade-off
- Compound scaling of depth, width, and resolution
- State-of-the-art performance
- Reasonable training time
```

#### 2. ResNet50

```
Parameters: ~25.6M
Advantages:
- Skip connections prevent vanishing gradients
- Deep architecture (50 layers)
- Proven performance on medical images
- More parameters for complex patterns
```

#### 3. VGG16

```
Parameters: ~138M
Advantages:
- Simple, sequential architecture
- Easy to understand and visualize
- Good baseline performance
Disadvantages:
- Large number of parameters
- Slower training
```

#### 4. MobileNetV2

```
Parameters: ~3.5M
Advantages:
- Extremely fast inference
- Small model size
- Good for deployment on mobile/edge devices
- Depthwise separable convolutions
```

### Custom Classification Head

```python
Base Model (Frozen/Fine-tuned)
    â†“
Global Average Pooling
    â†“
Batch Normalization
    â†“
Dense(512, ReLU)
    â†“
Dropout(0.5)
    â†“
Dense(256, ReLU)
    â†“
Batch Normalization
    â†“
Dropout(0.3)
    â†“
Dense(4, Softmax)  # Output probabilities for 4 classes
```

### Key Design Decisions

**Global Average Pooling:**
- Reduces spatial dimensions while preserving features
- Much fewer parameters than Flatten
- More robust to spatial translations

**Batch Normalization:**
- Normalizes layer inputs
- Stabilizes training
- Allows higher learning rates
- Acts as regularization

**Dropout:**
- Prevents overfitting
- Forces network to learn robust features
- Different dropout rates (0.5, 0.3) for different layers

**ReLU Activation:**
- Non-linearity for learning complex patterns
- No vanishing gradient problem
- Computationally efficient

**Softmax Output:**
- Converts logits to probabilities
- Outputs sum to 1.0
- Ideal for multi-class classification

## ğŸ“ Training Pipeline

### Two-Stage Training Strategy

Our training employs a two-stage approach for optimal results:

#### Stage 1: Feature Extraction (20-30 epochs)

```python
# Freeze base model weights
base_model.trainable = False

# Train only classification head
# Learning Rate: 0.001
# Optimizer: Adam
```

**Purpose:**
- Let new classification layers learn appropriate features
- Prevent destroying pre-trained weights
- Faster convergence

**Expected Behavior:**
- Rapid initial improvement
- Training accuracy reaches ~85-90%
- Validation accuracy follows training

#### Stage 2: Fine-Tuning (10-20 epochs)

```python
# Unfreeze last N layers
for layer in base_model.layers[-20:]:
    layer.trainable = True

# Fine-tune with lower learning rate
# Learning Rate: 0.0001
# Optimizer: Adam
```

**Purpose:**
- Adapt pre-trained features to medical images
- Learn domain-specific patterns
- Squeeze out final accuracy points

**Expected Behavior:**
- Slower but steady improvement
- Final accuracy reaches ~95%+
- May see slight overfitting (normal)

### Loss Function

**Categorical Cross-Entropy:**

```
L = -Î£áµ¢ yáµ¢ log(Å·áµ¢)
```

Where:
- yáµ¢: True label (one-hot encoded)
- Å·áµ¢: Predicted probability

**Why This Loss?**
- Standard for multi-class classification
- Penalizes wrong predictions heavily
- Differentiable (enables gradient descent)
- Probabilistic interpretation

### Optimization

**Adam Optimizer:**
- Adaptive learning rates per parameter
- Combines momentum and RMSprop
- Beta1 = 0.9 (momentum)
- Beta2 = 0.999 (adaptive LR)
- Epsilon = 1e-8 (numerical stability)

**Learning Rate Schedule:**
- ReduceLROnPlateau callback
- Reduces LR by 0.5 when validation loss plateaus
- Patience: 5 epochs
- Minimum LR: 1e-7

### Regularization Techniques

1. **Dropout (0.5, 0.3)**
   - Randomly deactivates neurons during training
   - Prevents co-adaptation of features

2. **Batch Normalization**
   - Normalizes layer inputs
   - Reduces internal covariate shift

3. **Data Augmentation**
   - Rotation, zoom, flip
   - Artificially increases dataset size

4. **Early Stopping**
   - Monitors validation loss
   - Stops if no improvement for 10 epochs
   - Restores best weights

5. **L2 Regularization** (implicit in Adam)
   - Penalizes large weights
   - Encourages simpler models

### Training Callbacks

```python
ModelCheckpoint: Save best model
EarlyStopping: Prevent overfitting
ReduceLROnPlateau: Adaptive learning rate
TensorBoard: Visualization and logging
```

## ğŸ’» Web Application

### Features

**1. Home Page**
- Project overview
- Key features and capabilities
- Quick statistics
- Learning path overview

**2. Detection Page**
- Image upload interface
- Real-time prediction
- Confidence scores for all classes
- Detailed pathology descriptions
- Interactive visualizations

**3. About Model Page**
- Architecture details
- Training strategy explanation
- Performance metrics
- Dataset information

**4. Theory Page**
- Neural networks fundamentals
- CNN architecture explained
- Transfer learning concepts
- Optimization algorithms
- Evaluation metrics

### User Interface Design

**Color Scheme:**
- Primary: Blue (#1f77b4) - Professional, trustworthy
- Success: Green (#28a745) - Positive results
- Warning: Yellow (#ffc107) - Low confidence alerts
- Gradient backgrounds for emphasis

**Layout:**
- Responsive design (works on mobile)
- Two-column layouts for efficiency
- Card-based information presentation
- Interactive Plotly charts

**User Experience:**
- Clear navigation sidebar
- Progress indicators during processing
- Informative error messages
- Tooltips and help text
- Visual feedback for all actions

## ğŸ¯ Deep Learning Concepts Covered

This project provides comprehensive coverage of deep learning fundamentals:

### 1. Neural Networks
- Artificial neurons and perceptrons
- Activation functions (ReLU, Sigmoid, Softmax)
- Forward propagation
- Backpropagation and gradient descent
- Universal approximation theorem

### 2. Convolutional Neural Networks
- Convolution operation
- Filters and feature maps
- Pooling layers (max, average)
- Translation invariance
- Hierarchical feature learning

### 3. Transfer Learning
- Pre-training on large datasets
- Feature extraction vs fine-tuning
- Domain adaptation
- When and why to use transfer learning

### 4. Optimization
- Gradient descent variants (SGD, Mini-batch)
- Adam optimizer
- Learning rate schedules
- Convergence and local minima

### 5. Regularization
- Overfitting vs underfitting
- Dropout
- Batch normalization
- Data augmentation
- L1/L2 regularization
- Early stopping

### 6. Model Evaluation
- Accuracy, Precision, Recall, F1-Score
- Confusion matrix
- ROC curves and AUC
- Cross-validation
- Calibration

### 7. Practical Deep Learning
- Data preprocessing and augmentation
- Batch processing
- Model checkpointing
- Hyperparameter tuning
- Debugging neural networks

## ğŸ“ Project Structure

```
neuropathology-dl/
â”œâ”€â”€ app.py                          # Streamlit web application
â”œâ”€â”€ train.py                        # Model training script
â”œâ”€â”€ requirements.txt                # Python dependencies
â”œâ”€â”€ README.md                       # This file
â”œâ”€â”€ .gitignore                     # Git ignore rules
â”‚
â”œâ”€â”€ src/                           # Source code
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â””â”€â”€ neuropathology_model.py # Model architecture
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â””â”€â”€ data_loader.py         # Data loading and preprocessing
â”‚   â””â”€â”€ utils/
â”‚       â””â”€â”€ helpers.py             # Utility functions
â”‚
â”œâ”€â”€ models/                        # Saved models (gitignored)
â”‚   â”œâ”€â”€ .gitkeep
â”‚   â””â”€â”€ final_model.h5
â”‚
â”œâ”€â”€ assets/                        # Images and resources
â”‚
â””â”€â”€ logs/                          # Training logs (gitignored)
```

## ğŸ“ˆ Results

### Expected Performance

With proper training on a good dataset:

| Metric | Value |
|--------|-------|
| Overall Accuracy | ~95% |
| Precision (avg) | ~94% |
| Recall (avg) | ~93% |
| F1-Score (avg) | ~94% |
| AUC-ROC | ~96% |

### Per-Class Performance

| Class | Precision | Recall | F1-Score |
|-------|-----------|--------|----------|
| Glioma | ~96% | ~94% | ~95% |
| Meningioma | ~93% | ~92% | ~93% |
| Pituitary | ~95% | ~94% | ~94% |
| Normal | ~92% | ~93% | ~93% |

### Training Time

On a typical setup:
- **With GPU (8GB+)**: 2-4 hours for complete training
- **CPU only**: 12-24 hours (not recommended)

### Model Size

- **EfficientNetB0**: ~21 MB (saved model)
- **ResNet50**: ~98 MB
- **VGG16**: ~528 MB
- **MobileNetV2**: ~14 MB

## ğŸ”® Future Work

### Planned Enhancements

1. **Model Improvements**
   - Ensemble methods (combining multiple models)
   - Attention mechanisms for interpretability
   - Self-supervised pre-training on medical images
   - 3D CNN for volumetric MRI analysis

2. **Feature Additions**
   - Grad-CAM visualizations for explainability
   - Uncertainty quantification
   - Multi-modal fusion (T1, T2, FLAIR sequences)
   - Tumor segmentation capabilities
   - Longitudinal analysis (tracking over time)

3. **Deployment**
   - Docker containerization
   - REST API for integration
   - Mobile application
   - Cloud deployment (AWS/GCP/Azure)
   - Edge deployment for offline use

4. **User Experience**
   - DICOM file support
   - Batch processing
   - Report generation (PDF)
   - Integration with PACS systems
   - Multi-language support

5. **Research Extensions**
   - Federated learning for privacy
   - Active learning for efficient labeling
   - Few-shot learning for rare conditions
   - Adversarial robustness testing

## ğŸ¤ Contributing

Contributions are welcome! Here's how you can help:

### Areas for Contribution

1. **Code Improvements**
   - Bug fixes
   - Performance optimizations
   - Code refactoring
   - Test coverage

2. **Documentation**
   - Tutorial improvements
   - Theory explanations
   - Code comments
   - Usage examples

3. **Features**
   - New model architectures
   - Additional visualizations
   - UI enhancements
   - Data preprocessing techniques

### Contribution Guidelines

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

### Code Style

- Follow PEP 8 guidelines
- Add docstrings to all functions
- Include type hints where appropriate
- Write unit tests for new features

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

## âš ï¸ Disclaimer

This system is designed for **educational and research purposes only**. It should not be used as the sole basis for medical diagnosis or treatment decisions. Always consult qualified healthcare professionals for medical advice.

Key Points:
- Not FDA approved or clinically validated
- No warranty or guarantee of accuracy
- Not a substitute for professional medical diagnosis
- Use at your own risk
- Intended for learning and research

## ğŸ™ Acknowledgments

- **TensorFlow/Keras Team**: For the excellent deep learning framework
- **Streamlit Team**: For the intuitive web app framework
- **ImageNet**: For pre-trained model weights
- **Kaggle**: For hosting medical imaging datasets
- **Open Source Community**: For inspiration and resources

## ğŸ“§ Contact

For questions, suggestions, or collaboration opportunities:

- GitHub: [@amanraj41](https://github.com/amanraj41)
- Project Link: [https://github.com/amanraj41/neuropathology-dl](https://github.com/amanraj41/neuropathology-dl)

## ğŸŒŸ Star History

If you find this project helpful, please consider giving it a star â­!

---

**Built with â¤ï¸ for learning deep learning from scratch**